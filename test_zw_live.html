<!DOCTYPE html>
<html lang="en">
<head>
    <title>webvr live-video</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <style>
        body {
            font-family: Monospace;
            background-color: #f0f0f0;
            margin: 0px;
            overflow: hidden;
        }
        #info {
            position: absolute;
            top: 10px;
            width: 100%;
            color: #fff;
            text-align: center;
        }
        a {
            color: #ff0
        }
    </style>
</head>
<body>
<div id="container"></div>
<!--
<div id="info">
	<a href="http://threejs.org" target="_blank">three.js</a> webgl - vr video<br />
	stereoscopic panoramic render by <a href="http://pedrofe.com/rendering-for-oculus-rift-with-arnold/" target="_blank">pedrofe</a>. scene from <a href="http://www.meryproject.com/" target="_blank">mery project</a>.
</div>
-->

<script src="static/js/three.js"></script>

<script src="static/js/VREffect.js"></script>
<script src="static/js/VRControls.js"></script>
<!-- A polyfill for the WebVR API.  -->
<script src="static/js/webvr-polyfill.js"></script>
<!-- 特效 -->
<script src="static/js/texiao.js"></script>
<!-- 视控 -->
<script src="static/js/EyeControls.js"></script>
<!-- 菜单 -->
<script src="static/js/vr_menu.js"></script>
<!-- 图形 -->
<script src="static/js/Geometry.js"></script>
<script src="static/js/HControl.js"></script>
<!-- 图像识别-->
<!--<script src="static/js/CvControls.js"></script>-->
<!-- 排行榜-->
<script src="static/js/rank.js"></script>
<!-- worker and websocket-->
<script src="static/js/VrWorker.js"></script>
<!--<script src="static/js/VrSocket.js"></script>-->
<script src="static/js/VrMessage.js"></script>


<script type="text/javascript">
    var user = false;

    var camera, scene, renderer;
    var video, texture;

    var controls, effect;
    var eye_controler;
    var menu;
    var hc = new HControl();     //头势控制&&菜单随动
    var thing;
    var vm;
    var ambientLight;
    var font;
    var rankStatus = false;
    var rank_data;
    var loader = new THREE.FontLoader();
    loader.load( 'static/json/FZYaoTi_Regular.json', function ( response ) {
        font = response;
        init_ws();
    } );
    var voice_status = false;

    /**
     * @author mrdoob / http://mrdoob.com
     * Based on @tojiro's vr-samples-utils.js
     */

    var vrDisplay;
    var WEBVR = {

        isLatestAvailable: function () {

            return navigator.getVRDisplays !== undefined;

        },

        isAvailable: function () {

            return navigator.getVRDisplays !== undefined || navigator.getVRDevices !== undefined;

        },

        getMessage: function () {

            var message;

            if ( navigator.getVRDisplays ) {

                navigator.getVRDisplays().then( function ( displays ) {

                    if ( displays.length === 0 ) message = 'WebVR supported, but no VRDisplays found.';

                } );

            } else if ( navigator.getVRDevices ) {

                message = 'Your browser supports WebVR but not the latest version. See <a href="http://webvr.info">webvr.info</a> for more info.';

            } else {

                message = 'Your browser does not support WebVR. See <a href="http://webvr.info">webvr.info</a> for assistance.';

            }

            if ( message !== undefined ) {

                var container = document.createElement( 'div' );
                container.style.position = 'absolute';
                container.style.left = '0';
                container.style.top = '0';
                container.style.right = '0';
                container.style.zIndex = '999';
                container.align = 'center';

                var error = document.createElement( 'div' );
                error.style.fontFamily = 'sans-serif';
                error.style.fontSize = '16px';
                error.style.fontStyle = 'normal';
                error.style.lineHeight = '26px';
                error.style.backgroundColor = '#fff';
                error.style.color = '#000';
                error.style.padding = '10px 20px';
                error.style.margin = '50px';
                error.style.display = 'inline-block';
                error.innerHTML = message;
                container.appendChild( error );

                return container;

            }

        },

        getButton: function () {

            var button = document.createElement( 'button' );
            button.style.position = 'absolute';
            button.style.left = 'calc(50% - 50px)';
            button.style.bottom = '20px';
            button.style.width = '100px';
            button.style.border = '0';
            button.style.padding = '8px';
            button.style.cursor = 'pointer';
            button.style.backgroundColor = '#000';
            button.style.color = '#fff';
            button.style.fontFamily = 'sans-serif';
            button.style.fontSize = '13px';
            button.style.fontStyle = 'normal';
            button.style.textAlign = 'center';
            button.style.zIndex = '999';
            button.textContent = 'ENTER VR';
            button.onclick = function() {
                console.log('vr model');
                //vrDisplay.isPresenting ? vrDisplay.exitPresent() : vrDisplay.requestPresent([{source: renderer.domElement}]);
                vrDisplay.requestPresent([{source: renderer.domElement}]);
                //init_webvr();

            };

            window.addEventListener( 'vrdisplaypresentchange', function ( event ) {

                button.textContent = effect.isPresenting ? 'EXIT VR' : 'ENTER VR';

            }, false );

            return button;

        }

    };



</script>

<script>

    if ( WEBVR.isLatestAvailable() === false ) {

        document.body.appendChild( WEBVR.getMessage() );

    }

    //

    init();
    animate();


    function init() {

        var container = document.getElementById( 'container' );
        container.addEventListener( 'click', function () {
            video.play();
        } );

        camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 1, 2000 );
        //camera.layers.enable( 1 ); // render left view when no stereo available
        eye_controler = new THREE.EyeControls(camera);
        // video

        video = document.createElement( 'video' );
        //print_video_event();
        //video.loop = true;
        //video.muted = true;

        //http://58.135.196.138:8090/live/789E9CE292A040d2A459CF55D2FB362E/789E9CE292A040d2A459CF55D2FB362E.m3u8

        //video.src = 'http://sohow.applinzi.com/static/img/sintel.mp4'
        //video.src = 'http://sohow.applinzi.com/static/img/MaryOculus.mp4';
        //video.src = 'http://sohow.applinzi.com/static/img/m1.mp4';
        //video.src = 'http://sohow.applinzi.com/static/img/hsy.mp4';
        //video.src = 'http://10.235.24.45/hls/test.m3u8';
        //video.src = 'http://58.135.196.138:8090/live/789E9CE292A040d2A459CF55D2FB362E/789E9CE292A040d2A459CF55D2FB362E.m3u8';
        //video.src = 'http://hls.yy.com/live/54880976_54880976.m3u8?id=yyent&uid=0d268e721bafa16011ed420182ecd1d81472019973&appid=15012&uuid=1A3512763E4046869071355677D1FD4A';
        //video.src = 'https://sohow.applinzi.com/static/video/v2.mp4';
        //video.src = 'http://10.235.24.45/hls/test.m3u8';
        video.src = 'https://10.235.24.45:8091/webvr/static/video/cv.mp4';
        //video.src = 'static/video/zp.mp4';
        video.setAttribute('crossorigin', 'anonymous');
        //video.setAttribute( 'type', 'application/vnd.apple.mpegurl' );
        //video.setAttribute( 'type', 'application/x-mpegURL' );	//video/mp4
        video.setAttribute( 'webkit-playsinline', 'webkit-playsinline' );
        video.load();
        video.play();

//        video.addEventListener('playing',function(){
//            if (typeof window.cv_controler == "undefined") {
//                window.cv_controler = new CvControls(video,camera,'static/js/JSARToolKit-worker.js');
//            }
//        });


        texture = new THREE.VideoTexture( video );
        texture.minFilter = THREE.NearestFilter;
        texture.maxFilter = THREE.NearestFilter;
        texture.format = THREE.RGBFormat;
        texture.generateMipmaps = false;

        scene = new THREE.Scene();

        material = new THREE.MeshLambertMaterial( { map: texture } );
        var mesh = new THREE.Mesh( get_geometry(), material );
        mesh.rotation.y = - Math.PI / 2;
        scene.add( mesh );

        //add_menu();
        menu = new THREE.VrMenu();

        var is_play = false;
        var is_zan = false;
        menu.add_option(scene, eye_controler, 'static/img/love.png', function (obj) {
            if (!is_zan ){
                is_zan = true;
                zan(10);
                setTimeout(function(){is_zan = false},300*10);
            }
        });
        menu.add_option(scene, eye_controler, 'static/img/f.png', function (obj) {
//            var data= {title:"123",items:["11","22","33"]}
//            addBox(data);
//            playAudio("http://lizhilei.cn/webvr/static/audio/th.wav");
//            placeItem();
//            showTips('你好');
            if (!is_play ){
                is_play = true;
                texiao1();
                setTimeout(function(){is_play = false},1500)
            }
        });
        menu.add_option(scene, eye_controler, 'static/img/yl.png', function (obj) {
            if (!is_play ){
                is_play = true;
                texiao();
                setTimeout(function(){is_play = false},1000)
            }
        });

        menu.add_option(scene, eye_controler, 'static/img/voice.png', function (obj) {
            voice();
        });

//        menu.add_option(scene, eye_controler, 'static/img/cview_close.png', function (obj) {
//            window.cv_controler.switch();
//            var str = window.cv_controler.paused ? 'close' : 'open';
//            menu.change_pic(obj,'static/img/cview_'+str+'.png');
//        });

        menu.add_option(scene, eye_controler, 'static/img/rank.png', function (obj) {
            rankStatus = ~rankStatus;
            showRank(rank_data);
        });

        menu.show(scene,eye_controler);

        //

        renderer = new THREE.WebGLRenderer();
        renderer.setClearColor( 0x101010 );
        renderer.setPixelRatio( window.devicePixelRatio );
        renderer.setSize( window.innerWidth, window.innerHeight );
        container.appendChild( renderer.domElement );

        //

        controls = new THREE.VRControls( camera );

        effect = new THREE.VREffect( renderer );
        effect.scale = 0; // video doesn't need eye separation
        effect.setSize( window.innerWidth, window.innerHeight );

        if ( WEBVR.isAvailable() === true ) {
            // Get the VRDisplay and save it for later.
            navigator.getVRDisplays().then(function(displays) {
                if (displays.length > 0) {
                    vrDisplay = displays[0];
                }
            });
            document.body.appendChild( WEBVR.getButton(  ) );

        }

        ambientLight = new THREE.AmbientLight(0xffffff  );
        scene.add( ambientLight );
        hc.open();
        //
        window.addEventListener( 'resize', onWindowResize, false );


    }


    function onWindowResize() {

        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();

        effect.setSize( window.innerWidth, window.innerHeight );

    }



    function animate() {

        render();
        requestAnimationFrame( animate );

    }

    function render() {

        controls.update();
        menu.update(hc);
        eye_controler.update();
        effect.render( scene, camera );

    }

    function cv_found() {
        cv.hide_cursor(scene);
    }


    function get_geometry() {
        var geometry = new THREE.SphereBufferGeometry( 2000, 60, 40 ).toNonIndexed();
        geometry.scale( - 1, 1, 1 );

        // Remap UVs

//        var normals = geometry.attributes.normal.array;
//        var uvs = geometry.attributes.uv.array;
//
//        for ( var i = 0, l = normals.length / 3; i < l; i ++ ) {
//
//            var x = normals[ i * 3 + 0 ];
//            var y = normals[ i * 3 + 1 ];
//            var z = normals[ i * 3 + 2 ];
//
//            if ( i < l / 2 ) {
//
//                var correction = ( x == 0 && z == 0 ) ? 1 : ( Math.acos( y ) / Math.sqrt( x * x + z * z ) ) * ( 2 / Math.PI );
//                uvs[ i * 2 + 0 ] = x * ( 404 / 1920 ) * correction + ( 447 / 1920 );
//                uvs[ i * 2 + 1 ] = z * ( 404 / 1080 ) * correction + ( 582 / 1080 );
//
//            } else {
//
//                var correction = ( x == 0 && z == 0 ) ? 1 : ( Math.acos( - y ) / Math.sqrt( x * x + z * z ) ) * ( 2 / Math.PI );
//                uvs[ i * 2 + 0 ] = - x * ( 404 / 1920 ) * correction + ( 1460 / 1920 );
//                uvs[ i * 2 + 1 ] = z * ( 404 / 1080 ) * correction + ( 582 / 1080 );
//
//            }
//
//        }

        //geometry.rotateZ(  Math.PI / 2 );

        return geometry;
        //
    }

</script>
<script>
    // These will be initialized later
    var recognizer, recorder, callbackManager, audioContext, outputContainer;
    // Only when both recorder and recognizer do we have a ready application
    var isRecorderReady = isRecognizerReady = false;

    // A convenience function to post a message to the recognizer and associate
    // a callback to its response
    function postRecognizerJob(message, callback) {
        var msg = message || {};
        if (callbackManager) msg.callbackId = callbackManager.add(callback);
        if (recognizer) recognizer.postMessage(msg);
    };

    // This function initializes an instance of the recorder
    // it posts a message right away and calls onReady when it
    // is ready so that onmessage can be properly set
    function spawnWorker(workerURL, onReady) {
        recognizer = new Worker(workerURL);
        recognizer.onmessage = function(event) {
            onReady(recognizer);
        };
        // Notice that we pass the name of the pocketsphinx.js file to load
        // as we need the file packaged with the Chinese acoustic model
        recognizer.postMessage('pocketsphinx_zh.js');
    };

    // To display the hypothesis sent by the recognizer
    function updateHyp(hyp) {
        switch(hyp){
            case '送鲜花':
                texiao1();
                break;
            case '送邮轮':
                texiao();
                break;
            case '点个赞':
                zan(10);
                break;
            default:
                break;
        }
    };

    function checkLunch() {
        if (isRecorderReady && isRecognizerReady){
            recorder && recorder.start(0)
        }
    };

    // This is just a logging window where we display the status
    function updateStatus(newStatus) {
        //document.getElementById('current-status').innerHTML += "<br/>" + newStatus;
    };

    // Callback function once the user authorises access to the microphone
    // in it, we instanciate the recorder
    function startUserMedia(stream) {
        var input = audioContext.createMediaStreamSource(stream);
        // Firefox hack https://support.mozilla.org/en-US/questions/984179
        window.firefox_audio_hack = input;
        // Notice that for this Chinese acoustic model, audio must be at 8kHz, so we should
        // request it from the recorder
        var audioRecorderConfig = {
            errorCallback: function(x) {updateStatus("Error from recorder: " + x);},
            outputSampleRate: 8000
        };
        recorder = new AudioRecorder(input, audioRecorderConfig);
        // If a recognizer is ready, we pass it to the recorder
        if (recognizer) recorder.consumers = [recognizer];
        isRecorderReady = true;
        checkLunch();
        updateStatus("Audio recorder ready");
    };

    // Called once the recognizer is ready
    // We then add the grammars to the input select tag and update the UI
    var recognizerReady = function() {
        isRecognizerReady = true;
        checkLunch();
        updateStatus("Recognizer ready");
    };

    // This adds a grammar from the grammars array
    // We add them one by one and call it again as
    // a callback.
    // Once we are done adding all grammars, we can call
    // recognizerReady()
    var feedGrammar = function(g, index, id) {
        if (id && (grammarIds.length > 0)) grammarIds[0].id = id.id;
        if (index < g.length) {
            grammarIds.unshift({title: g[index].title})
            postRecognizerJob({command: 'addGrammar', data: g[index].g},
                    function(id) {feedGrammar(grammars, index + 1, {id:id});});
        } else {
            recognizerReady();
        }
    };

    // This adds words to the recognizer. When it calls back, we add grammars
    var feedWords = function(words) {
        postRecognizerJob({command: 'addWords', data: words},
                function() {feedGrammar(grammars, 0);});
    };

    // This initializes the recognizer. When it calls back, we add words
    var initRecognizer = function() {
        // You can pass parameters to the recognizer, such as : {command: 'initialize', data: [["-hmm", "my_model"], ["-fwdflat", "no"]]}
        // Pay attention here to state the sample rate as the default value is 16kHz and this Chinese acoustic model uses 8kHz
        postRecognizerJob({command: 'initialize', data:[["-samprate", "8000"]]},
                function() {
                    if (recorder) recorder.consumers = [recognizer];
                    feedWords(wordList);});
    };

    function open_voice() {
        outputContainer = document.getElementById("output");
        updateStatus("Initializing web audio and speech recognizer, waiting for approval to access the microphone");
        callbackManager = new CallbackManager();
        spawnWorker("js/recognizer.js", function(worker) {
            // This is the onmessage function, once the worker is fully loaded
            worker.onmessage = function(e) {
                // This is the case when we have a callback id to be called
                if (e.data.hasOwnProperty('id')) {
                    var clb = callbackManager.get(e.data['id']);
                    var data = {};
                    if ( e.data.hasOwnProperty('data')) data = e.data.data;
                    if(clb) clb(data);
                }
                // This is a case when the recognizer has a new hypothesis
                // Notice that pocketsphinx.js does not yet recognize words
                // encoded in UTF8, so we map them to ASCII strings. Here we
                // display both ASCII and Chinese strings
                if (e.data.hasOwnProperty('hyp')) {
                    console.log(e.data.hyp);
                    var hyparray = e.data.hyp.split(' ');
                    if(hyparray[hypNum] != '' && hyparray.length>hypNum){
                        var newHyp = hyparray[hypNum];
                        hypNum++;
                        var newHypChinese = wordListChinese[newHyp];
                        updateHyp(newHypChinese);
                    }
                }
                // This is the case when we have an error
                if (e.data.hasOwnProperty('status') && (e.data.status == "error")) {
                    updateStatus("Error in " + e.data.command + " with code " + e.data.code);
                }
            };
            // Once the worker is fully loaded, we can call the initialize function
            initRecognizer();
        });

        // The following is to initialize Web Audio
        try {
            window.AudioContext = window.AudioContext || window.webkitAudioContext;
            navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
            window.URL = window.URL || window.webkitURL;
            audioContext = new AudioContext();
        } catch (e) {
            updateStatus("Error initializing Web Audio browser");
        }
        if (navigator.getUserMedia) navigator.getUserMedia({audio: true}, startUserMedia, function(e) {
            updateStatus("No live audio input in this browser");
        });
        else updateStatus("No web audio support in this browser");
    }
    // When the page is loaded, we spawn a new recognizer worker and call getUserMedia to
    // request access to the microphone
    window.init_ws = function() {
        //init WebSocket
        if (vm) {
            return false;
        }

        window.vr_onmessage = function(evt) {
            var data = JSON.parse(evt);
            console.log(data);
            if (data.type == "video"){
                changeVideo(data.data);
            } else if(data.type == "menu") {
                addBox(data.data);
            } else if(data.type == "item") {
                placeItem(data.data);
            } else if(data.type == "music") {
                playAudio(data.data);
            } else if(data.type == "uid") {
                user = data.data;
            } else if(data.type == "delitem") {
                delItem(data);
            } else if(data.type == "headmenu") {
                head(data.data);
            } else if(data.type == "tips") {
                showTips(data.data);
            } else if(data.type == "rank") {
                showRank(data.data);
            } else if(data.type == "gift") {
                if (data.data.uid != user) {
                    if (data.data.gift_type == vm.GIFT_TYPE.FLOWER) {
                        texiao1(false);
                    } else if (data.data.gift_type == vm.GIFT_TYPE.BOAT) {
                        texiao(false);
                    }
                }
            }

        };
        vm = new VrMessage('static/js/VrSocket.js','wss://10.235.24.45:8091/user',window.vr_onmessage);

    };

    // This is the list of words that need to be added to the recognizer
    // This follows the CMU dictionary format and the phone set of the Chinese model
    // These words were taken from model/lm/zh_CN/mandarin_notone.dic
    var wordList = [["dian_ge_zan","d i an g e z an"], ["song_you_lun", "s ong y ou l un"], ["song_xian_hua", "s ong x i an h ua"]];
    var wordListChinese = {"dian_ge_zan": "点个赞", "song_you_lun": "送邮轮", "song_xian_hua": "送鲜花"};
    var grammarChineseGreetings = {numStates: 1, start: 0, end: 0, transitions: [{from: 0, to: 0, word: "dian_ge_zan"},{from: 0, to: 0, word: "song_you_lun"},{from: 0, to: 0, word: "song_xian_hua"}]};
    var grammars = [{title: "Chinese Greetings", g: grammarChineseGreetings}];
    var grammarIds = [];
    var hypNum = 0;
</script>
<!-- These are the two JavaScript files you must load in the HTML,
    The recognizer is loaded through a Web Worker -->
<script src="js/audioRecorder.js"></script>
<script src="js/callbackManager.js"></script>
</body>
</html>
